Here’s a single, copy-paste prompt you can give to the Replit agent. It tells it exactly what to inspect, run, and produce so you get a clean status + artifacts on the AI contamination safeguards, old data handling, data structure, and AI flow.

---

**PROMPT FOR REPLIT AGENT (paste everything below):**

You are the repo assistant for **finbrain**. Act like a senior Google engineer. Do NOT ask me questions—perform the steps and return the outputs.

## Context

We’ve shipped CRITICAL AI cross-contamination safeguards. I need a definitive status + artifacts:

1. Are users protected from mixed data now?
2. What happens to old data/caches?
3. How is new data structured (DB + cache + data\_version)?
4. How exactly does the AI flow run now, end-to-end?

## Scope of Work

### A) Code Audit (read-only, then minor codegen if missing)

* Confirm these files exist and are wired in:

  * `utils/ai_adapter_v2.py`
  * `utils/ai_contamination_monitor.py`
  * `ai/payloads/insight_payload.py`
  * `presenters/insights_text.py`
  * `utils/cache.py`
  * Any metrics/logger utilities referenced
* Grep to ensure **banned generic lines** are gone:

  * “transport costs are significant”
  * “Food expenses are also high”
* Verify per-request session isolation: `ai_adapter_v2` must NOT hold a module-level shared client/session.
* Verify JSON-only model contract + Pydantic (or equivalent) validation and `_echo.user_id` in outputs.
* Verify cache isolation:

  * Keys include `{user_id}` and `data_version`
  * Read path asserts cached `_echo.user_id == expected_user_id`
* Verify payload builder sets `meta.insufficient_data=true` when totals=0.

If any item is missing, **create or patch** the minimal file/stub (without breaking existing imports) and show the diff.

### B) Tests & Static Checks

* Run a repo-wide grep to guarantee banned phrases are absent.
* Locate tests for tenancy/no-data. If missing, create `tests/test_insights_tenancy.py` with:

  * `test_zero_data_minimal_response()`
  * `test_no_cross_user_mix()` (two synthetic users; assert no cross amounts & `_echo.user_id` match)
* Run tests. If DB constraints fail, **skip DB-dependent tests** temporarily and keep tenancy/no-data tests using fakes/mocks. Show which tests were skipped and why.

### C) DB & Data Structure Summary

* Scan migrations and DB helpers to produce a concise spec:

  * Tables: `ledger`, `ledger_audit` (or equivalent), any `*_totals`, `insight_runs`, cache layer usage (Redis or in-memory)
  * Required columns (user\_id scoping, soft delete, idempotency, occurred\_at/updated\_at)
  * Indexes needed for `(user_id, occurred_at)` and `(user_id, updated_at)`
  * `data_version` definition (e.g., `MAX(updated_at)` across the user’s rows within window, or checksum)
* If audit or soft-delete migration is missing, **draft** a migration file named `migrations/20250827_contamination_audit.sql` with safe, idempotent SQL (wrapped in IF NOT EXISTS).

### D) Runtime Flow Trace (AI pipeline)

Generate an end-to-end sequence (bullet list) for:

1. Build insight payload → mark insufficient\_data when totals=0
2. Call `ai_adapter_v2` with system prompt (payload-only, no memory)
3. Schema validation → add `_echo.user_id`
4. `ai_contamination_monitor` checks output against span/user
5. Cache write/read with `user_id + window_hash + data_version`
6. Presenter renders → if insufficient\_data: single safe line
7. Metrics + logs emitted; alerts if mismatch/non-JSON/schema violation

### E) Operational Guidance (what happens now)

Produce an actionable checklist for:

* Purging legacy insight caches
* Rebuilding aggregates for active users
* Running in `PCA_MODE=SHADOW` for 24–48h (log diffs, show new safe output)
* Alert thresholds for:

  * `insights.tenant_mismatch_total`
  * `insights.adapter.schema_violation_total`
  * `insights.adapter.nonjson_total`
* UX nudge when a user has 0 data (exact string)

## Commands to Execute

1. Repo checks

   * Grep banned lines
   * List and open the files in Section A
2. Test run

   * Run the tenancy/no-data tests; if DB is required, use mocks or mark xfail and explain
3. Generate artifacts

   * If missing, create `tests/test_insights_tenancy.py`
   * If missing, create `migrations/20250827_contamination_audit.sql`
   * Create `docs/security/ai_contamination_final_report.md` (see Deliverables)

## Deliverables (return these in your reply)

1. **PASS/FAIL matrix** for Section A checks (each item with file/line pointers)
2. **Grep result** proving banned phrases are absent (or the exact files/lines if present)
3. **Test summary** (which ran, which skipped, exit code)
4. **DB/Data Structure spec**: concise table/column/index list + `data_version` rule
5. **AI Flow trace** (numbered steps)
6. **Ops Checklist** (cache purge, aggregates rebuild, SHADOW rollout, alerts, UX string)
7. If any code was added/edited: a **unified diff** snippet for each change
8. `docs/security/ai_contamination_final_report.md` containing:

   * Background, Root Cause (shared session, cache mismatch, unsafe fallbacks)
   * Safeguards implemented (adapter hardening, monitor, cache/user isolation, no-data handling, observability)
   * Current status (“either correct per-user or safe minimal”)
   * Old data policy (ledger preserved/restored; legacy insight caches purged)
   * New data structure (tables, keys, `data_version`)
   * AI contract (JSON-only, schema-validated, payload-only)
   * Rollout & Alerts
   * Residual risks & next steps (DB constraints to fix, admin health panel)

**Constraints**

* Don’t break the build. If DB is unavailable, mock what’s necessary and proceed.
* Prefer minimal, composable diffs.
* Be explicit—include file paths and line numbers in findings.

**Return all outputs in one message.**
