You’ve got two problems showing up in that log:

A stray runtime path still calling ensure_hashed without an import

The error is specifically from utils.production_router during AI handling:

AI expense logging error: name 'ensure_hashed' is not defined


That means a different process/module path (very likely your background worker) is loading another copy of the router or importing it differently, so your hotfix didn’t apply there.

Write/Read reordering in the same chat flow

The log clearly shows:

TRACE [summary_query] ... user_id_preview '52ff5a50...'

TRACE [summary_result] found_expenses: 0

then TRACE [record_expense] ... amount 100

Summary ran before the expense commit—so even with perfect hashing, the summary returns 0.

Here’s how to fix both, fast.

Fix-now checklist (do these in order)
A) Kill the last “ensure_hashed” error in every process

Log which router file each process loads.
At the top of the canonical utils/production_router.py:

import logging, pathlib, hashlib
_P = pathlib.Path(__file__).resolve()
logging.warning("PRODUCTION_ROUTER_INIT file=%s sha=%s",
                _P, hashlib.sha256(_P.read_bytes()).hexdigest()[:12])


Make sure this line shows up on web and worker startup. If it doesn’t appear for the worker, the worker is importing a different module.

Defensive import guard (keeps runtime alive even if import order is odd):

try:
    from utils.user_manager import resolve_user_id   # preferred single point
except Exception:
    from utils.crypto import ensure_hashed            # last-resort fallback
    def resolve_user_id(*, psid=None, psid_hash=None):
        return ensure_hashed(psid or psid_hash)


Standardize all imports to the same router path
Search for any non-canonical imports:

Bad: import production_router or from app.utils import production_router

Good: from utils.production_router import ...
Update every reference (web + worker).

Nuke duplicates & caches:

Remove any extra production_router.py copies.

find . -name "__pycache__" -type d -exec rm -rf {} +

Restart all processes (web + workers).

You should now stop seeing “ensure_hashed is not defined.”

B) Enforce write-before-read ordering (stop summaries beating writes)

Right now your logs show summary firing before the expense is actually recorded/committed.

Do one of these (start with #1 for UAT; add #2 for production):

Handle expense writes synchronously on the webhook thread
In your webhook handler, if the intent is an expense:

call user_manager.record_expense(...)

commit

then enqueue any AI work (if needed)

Don’t queue the write to a background worker during UAT—commit it inline so summary right after will see it.

Per-PSID FIFO queue for background work (production-hardening)
If you must background-process, enforce per-user ordering:

Key jobs by user_id.

A new job for the same user waits until the previous is done.

Or, simpler: put writes on the main thread; only AI/narrative generation is queued.

Add a tiny debounce in the summary route (optional)
If the last event was an expense within 250–500 ms, sleep 200 ms before reading to let the commit land. Not elegant, but effective during migration.

C) One copy-paste prompt to make these changes (Replit AI)

Paste everything between the lines:

You are the repo maintainer. Fix two issues: (1) lingering ensure_hashed NameError in the worker path due to duplicate/incorrect imports of production_router, and (2) summary sometimes runs before the expense commit.

Tasks

Identify and log the loaded router file

Ensure utils/production_router.py has:

import logging, pathlib, hashlib
_P = pathlib.Path(__file__).resolve()
logging.warning("PRODUCTION_ROUTER_INIT file=%s sha=%s",
                _P, hashlib.sha256(_P.read_bytes()).hexdigest()[:12])


Start both the web app and the background worker; capture the PRODUCTION_ROUTER_INIT lines for both.

Defensive import + centralized resolver

At top of utils/production_router.py add:

try:
    from utils.user_manager import resolve_user_id
except Exception:
    from utils.crypto import ensure_hashed
    def resolve_user_id(*, psid=None, psid_hash=None):
        return ensure_hashed(psid or psid_hash)


Grep for any direct ensure_hashed( calls outside utils/user_manager.py and replace with:

from utils.user_manager import resolve_user_id
user_id = resolve_user_id(psid=..., psid_hash=...)


Normalize imports

Grep for production_router imports. Replace all variants with:

from utils.production_router import <symbols>


Delete or rename any duplicate production_router.py files found; remove their __pycache__. Restart processes.

Synchronous expense writes (UAT mode)

In the webhook handler for expense intents, perform:

user_manager.record_expense(...); db.session.commit() inline

Only after commit, enqueue AI work or send Messenger ACK

Gate this behavior behind SYNC_EXPENSE_WRITES=true env flag.

Optional per-user FIFO (production)

In background processing, ensure tasks are serialized per user_id (queue keyed by user_id or advisory locks).

Verification

Run a Messenger canary: 120 groceries, 100 Uber, summary, insights.

Expect: no NameError logs, summary total ≥ 220.

Show:

The two PRODUCTION_ROUTER_INIT lines (web + worker) with the same file path and sha.

Last 8 TRACE lines: record_expense and summary_query with identical user_id.

Quickscan raw vs hash identical totals for the canary PSID.

Deliverables

Diffs for modified files

List of removed duplicate files

The two PRODUCTION_ROUTER_INIT lines

Canary logs + quickscan outputs

Be surgical; do not refactor unrelated code.