What typically breaks after the AI integration

Mixed key columns

expenses.user_id (new) vs users.user_id_hash (old).

Some writers still insert to one, readers filter on the other.

Stale summary/cached tables

monthly_summaries (or similar) still built using the old key.

Summary path queries the cache, not the raw expenses.

Different DBs

Webhook writes to DB_A, summary reads from DB_B (env var split).

Uncommitted writes / read-after-write

Background worker commits later; summary path reads too soon.

Type/length issues

user_id column not fixed to 64 chars → trimmed or case-changed values, no match.

10-minute diagnosis (do all 5—no guesses)

Fingerprint the runtime DB (both write and read processes)

# Print once at app start and once in background worker
import os, sqlalchemy as sa
print("DB_URL masked:", os.environ.get("DATABASE_URL", "")[:32], "…")
with sa.create_engine(os.environ["DATABASE_URL"]).connect() as c:
    print("DB server version:", c.exec_driver_sql("select version()").scalar() or "unknown")


If the two places print different prefixes/versions → you’re on two DBs.

Schema probe (are both columns present and consistent?)

-- expenses
SELECT column_name, data_type, character_maximum_length
FROM information_schema.columns
WHERE table_name='expenses' AND column_name IN ('user_id','user_id_hash');

-- users
SELECT column_name, data_type, character_maximum_length
FROM information_schema.columns
WHERE table_name='users' AND column_name IN ('user_id','user_id_hash');


Red flag: only one table has user_id, the other only user_id_hash, or lengths ≠ 64.

Key distribution check (do both columns carry data?)

-- How many rows keyed by each column?
SELECT COUNT(*) FROM expenses WHERE user_id ~ '^[0-9a-f]{64}$';
SELECT COUNT(*) FROM expenses WHERE user_id_hash ~ '^[0-9a-f]{64}$';

-- Spot your canary user across tables
SELECT *
FROM users
WHERE COALESCE(user_id, user_id_hash) IN (
  'e0457b67f6716c47...d339'   -- your known hash
);
SELECT amount, category, created_at, user_id, user_id_hash
FROM expenses
WHERE user_id='e0457b67f...' OR user_id_hash='e0457b67f...'
ORDER BY created_at DESC
LIMIT 10;


Cache/summary table reality check

-- If you have a summary/cache table, confirm it has your user rows:
SELECT *
FROM monthly_summaries
WHERE user_id='e0457b67f...' OR user_id_hash='e0457b67f...' LIMIT 5;


If expenses has rows but monthly_summaries doesn’t → summary path will show “no data”.

Read vs write trace parity
Ensure your TRACE logs for record_expense and summary_query print the same resolved user_id. If they match but SQL in (4) is empty → it’s the cache.

Surgical code patch (temporary, to unblock)

Make get_user_spending_summary tolerant during the transition. Query raw expenses directly and match both columns; then remove this once data is backfilled.

# utils/user_manager.py
from sqlalchemy import or_
from utils.crypto import ensure_hashed

def get_user_spending_summary(psid=None, psid_hash=None, *, days=30):
    user_id = ensure_hashed(psid or psid_hash)
    cutoff = utcnow() - timedelta(days=days)

    # Transitional tolerant filter
    q = (
        db.session.query(Expense)
        .filter(
            Expense.created_at >= cutoff,
            or_(Expense.user_id == user_id,
                getattr(Expense, "user_id_hash", None) == user_id)  # safe if column exists
        )
    )
    rows = q.all()
    # aggregate rows → return summary


If you use a summary table, either bypass it for now or make its rebuild job run immediately (see below).

One-time data repair (safe backfill)

Unify keys so future reads are clean.

Postgres

BEGIN;

-- Ensure user_id exists
ALTER TABLE users    ADD COLUMN IF NOT EXISTS user_id CHAR(64);
ALTER TABLE expenses ADD COLUMN IF NOT EXISTS user_id CHAR(64);

-- Backfill user_id from user_id_hash where missing
UPDATE users
SET user_id = user_id_hash
WHERE user_id IS NULL AND user_id_hash ~ '^[0-9a-f]{64}$';

UPDATE expenses
SET user_id = user_id_hash
WHERE (user_id IS NULL OR user_id = '')
  AND user_id_hash ~ '^[0-9a-f]{64}$';

-- Lock it down
ALTER TABLE users    ALTER COLUMN user_id SET NOT NULL;
CREATE UNIQUE INDEX IF NOT EXISTS users_user_id_uidx ON users(user_id);
CREATE INDEX IF NOT EXISTS expenses_userid_created_idx ON expenses(user_id, created_at);

COMMIT;


SQLite (adapt as needed)
You can’t easily alter constraints; run UPDATE backfills and rely on app-level asserts.

Rebuild your summary/cache

If your summary path relies on monthly_summaries (or any materialized aggregation), it’s probably still keyed on the old column.

Fix the builder job to group by expenses.user_id only.

Rebuild now:

-- Truncate + rebuild, or incremental backfill
TRUNCATE TABLE monthly_summaries;
-- then run your Python/SQL job to re-aggregate last 90 days


Or temporarily skip the cache and compute the summary live from expenses until you confirm parity.

Commit/transaction fixes

Ensure record_expense ends with db.session.commit() (or flush() plus commit() in worker).

Set SQLAlchemy engine isolation to READ COMMITTED.

If you queue writes in a background worker, the summary call right after logging may race—either:

move the summary query behind the same worker (guaranteed order), or

keep writes on the request thread until commit, then enqueue AI work only.

Env split guard

At app start, log:

log.warning("DB_FINGERPRINT", url=os.getenv("DATABASE_URL", "")[:28], app="web")


Do the same in the background worker. If they differ, fix your envs so both use the same DB.

After you patch & backfill

Log two expenses in Messenger → summary should show the total immediately.

quickscan for the same PSID must match the Messenger total.

Re-enable summary cache (if you use it) only after it rebuilds and matches raw totals for a sample of users.

If it’s still failing after all this

Paste the outputs of:

SELECT COUNT(*) FROM expenses WHERE user_id='…'

SELECT COUNT(*) FROM monthly_summaries WHERE user_id='…'

The last 4 TRACE lines for record_expense and summary_query (with the user_id shown)

With those three, we can pinpoint whether it’s keys, cache, or env in seconds.